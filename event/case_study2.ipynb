{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This case study investigates whether we can perform two branch parallel, but merging two branch\n",
    "1. Linear\n",
    "2. Matmul"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "def throughput(images, model):\n",
    "    model.eval()\n",
    "    batch_size, token_length = images[0].shape[0:2]\n",
    "    for i in range(50):\n",
    "        model(*images)\n",
    "    torch.cuda.synchronize()\n",
    "    tic1 = time.time()\n",
    "    for i in range(100):\n",
    "        model(*images)\n",
    "    torch.cuda.synchronize()\n",
    "    tic2 = time.time()\n",
    "    print(f\"batch_size {batch_size} token_length {token_length} throughput {100 * batch_size / (tic2 - tic1)}\")\n",
    "    MB = 1024.0 * 1024.0\n",
    "    print('memory:', torch.cuda.max_memory_reserved() / MB)\n",
    "    return (tic2 - tic1) / (100 * batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "class DummyModel(nn.Module):\n",
    "    def __init__(self, depth=12, embed_dim=768, stream=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = 768\n",
    "        self.stream = False\n",
    "        # self.model1 = nn.ModuleList([\n",
    "        #    nn.Linear(embed_dim, embed_dim) for _ in range(depth)])\n",
    "        # self.model2 = nn.ModuleList([\n",
    "        #    nn.Linear(embed_dim, embed_dim) for _ in range(depth)])\n",
    "        self.model1 = nn.Sequential(*[\n",
    "           nn.Linear(embed_dim, embed_dim) for _ in range(depth)])\n",
    "        self.model2 = nn.Sequential(*[\n",
    "           nn.Linear(embed_dim, embed_dim) for _ in range(depth)])\n",
    "\n",
    "        self.s1 = torch.cuda.Stream(device=device)\n",
    "        self.s2 = torch.cuda.Stream(device=device)\n",
    "    def forward(self, x1, x2):\n",
    "        if self.stream:\n",
    "            with torch.cuda.stream(self.s1):\n",
    "                x1 = self.model1(x1)\n",
    "            with torch.cuda.stream(self.s2):\n",
    "                x2 = self.model2(x2)\n",
    "        else:\n",
    "            for blk1, blk2 in zip(self.model1, self.model2):\n",
    "                x1 = blk1(x1)\n",
    "                x2 = blk2(x2)\n",
    "        return x1, x2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "class weight_fusion(DummyModel):\n",
    "    def __init__(self):\n",
    "        super(weight_fusion, self).__init__()\n",
    "        layers = []\n",
    "        for blk1, blk2 in zip(self.model1, self.model2):\n",
    "            weight = torch.zeros((2 * self.embed_dim, 2 * self.embed_dim))\n",
    "            #print(type(blk1.weight))\n",
    "            weight[:self.embed_dim, :self.embed_dim] = blk1.weight\n",
    "            weight[self.embed_dim:, self.embed_dim:] = blk2.weight\n",
    "            bias = torch.zeros(2 * self.embed_dim)\n",
    "            bias[:self.embed_dim] = blk1.bias\n",
    "            bias[self.embed_dim:] = blk2.bias\n",
    "            layer = nn.Linear(2 * self.embed_dim, 2 * self.embed_dim)\n",
    "            layer.weight = nn.Parameter(weight)\n",
    "            layer.bias = nn.Parameter(bias)\n",
    "            layers.append(layer)\n",
    "        self.model = nn.ModuleList(layers)\n",
    "    def forward(self, x):\n",
    "        for blk in self.model:\n",
    "            x = blk(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 40 token_length 400 throughput 607.9028967973745\n",
      "memory: 3638.0\n",
      "0.0016449995636940002\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "token_length = 400\n",
    "emb_dim = 768\n",
    "batch_size = 40\n",
    "non_blocking = True\n",
    "model = DummyModel(stream=False).to(device, non_blocking=non_blocking)\n",
    "data1 = torch.rand((batch_size, token_length, emb_dim)).to(device, non_blocking=non_blocking)\n",
    "data2 = torch.rand((batch_size, token_length, emb_dim)).to(device, non_blocking=non_blocking)\n",
    "latency = throughput((data1, data2), model)\n",
    "print(latency)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 4 token_length 10 throughput 1518.9369901708408\n",
      "memory: 2198.0\n",
      "batch_size 4 token_length 4 throughput 3333.3762492300966\n",
      "memory: 2198.0\n",
      "batch_size 4 token_length 0 throughput 9998.539502175252\n",
      "memory: 2198.0\n",
      "batch_size 4 token_length 0 throughput 9996.355114200596\n",
      "memory: 2198.0\n",
      "0.0002896005908648173\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "ratio = 0.4\n",
    "results = []\n",
    "token_length = 10\n",
    "for i in range(4):\n",
    "    token_length = int(token_length * (ratio ** i + 0.05))\n",
    "    model = DummyModel().to(device)\n",
    "    data1 = torch.rand((batch_size, token_length, emb_dim)).to(device)\n",
    "    data2 = torch.rand((batch_size, token_length, emb_dim)).to(device)\n",
    "    latency = throughput((data1, data2), model)\n",
    "    results.append(latency)\n",
    "print(sum(results)/4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 32 token_length 200 throughput 643.430974057145\n",
      "memory: 1510.0\n",
      "batch_size 32 token_length 80 throughput 1526.2333964441866\n",
      "memory: 1510.0\n",
      "batch_size 32 token_length 12 throughput 9320.093049739948\n",
      "memory: 1510.0\n",
      "batch_size 32 token_length 0 throughput 191995.60556933054\n",
      "memory: 1510.0\n",
      "0.0005804698914289475\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "ratio = 0.4\n",
    "results = []\n",
    "token_length = 100\n",
    "model = weight_fusion().to(device)\n",
    "for i in range(4):\n",
    "    token_length = int(token_length * (ratio ** i))\n",
    "    data1 = torch.rand((batch_size, token_length, emb_dim)).to(device)\n",
    "    data2 = torch.rand((batch_size, token_length, emb_dim)).to(device)\n",
    "    data = torch.zeros((batch_size, 2 * token_length, 2 * emb_dim)).to(device)\n",
    "    data[:, :token_length, :emb_dim] = data1\n",
    "    data[:, token_length:, emb_dim:] = data2\n",
    "    latency = throughput([data], model)\n",
    "    results.append(latency)\n",
    "print(sum(results)/4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import sparselinear as sl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sl1 = sl.SparseLinear(20000, 20000, sparsity=.99).cuda()\n",
    "# Reduce weight dimensions if memory errors are raised\n",
    "fc1 = nn.Linear(20000, 20000).cuda()\n",
    "x = torch.rand(20000, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%timeit y = sl1(x)\n",
    "%timeit y = fc1(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0,   1,   2,  ..., 197, 198, 199],\n        [  0,   0,   0,  ..., 199, 199, 199]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 100\n",
    "# num_connections = 4\n",
    "input_dim = 2 * emb_dim\n",
    "output_dim = 2 * emb_dim\n",
    "col = torch.arange(input_dim).repeat_interleave(emb_dim).view(1,-1).long()\n",
    "row = torch.cat([torch.arange(emb_dim).repeat(emb_dim).view(1,-1), torch.arange(emb_dim, 2*emb_dim).repeat(emb_dim).view(1,-1)], dim=1)\n",
    "# row = torch.randint(low=0, high=output_dim, size=(input_dim*num_connections,)).view(1,-1).long()\n",
    "connections = torch.cat((row, col), dim=0)\n",
    "connections"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.7 ms ± 541 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "89.8 µs ± 605 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_sparse import spspmm\n",
    "device = 'cuda'\n",
    "\n",
    "emb_dim = 300\n",
    "col = torch.arange(emb_dim * 2).repeat_interleave(emb_dim).view(1,-1).long()\n",
    "row = torch.cat([torch.arange(emb_dim).repeat(emb_dim).view(1,-1), torch.arange(emb_dim, 2*emb_dim).repeat(emb_dim).view(1,-1)], dim=1)\n",
    "\n",
    "indexA = torch.cat((col, row), dim=0).to(device)\n",
    "valueA = torch.rand(2 * emb_dim * emb_dim).to(device)\n",
    "\n",
    "indexB = torch.cat((col, row), dim=0).to(device)\n",
    "valueB = torch.rand(2 * emb_dim * emb_dim).to(device)\n",
    "\n",
    "\n",
    "\n",
    "matrixA = torch.zeros((2 * emb_dim, 2 * emb_dim)).to(device)\n",
    "dataA = valueA.reshape(emb_dim, -1)\n",
    "matrixA[:emb_dim, :emb_dim] = dataA[:, :emb_dim]\n",
    "matrixA[emb_dim:, emb_dim:] = dataA[:, emb_dim:]\n",
    "\n",
    "matrixB = torch.zeros((2 * emb_dim, 2 * emb_dim)).to(device)\n",
    "dataB = valueB.reshape(emb_dim, -1)\n",
    "matrixB[:emb_dim, :emb_dim] = dataB[:, :emb_dim]\n",
    "matrixB[emb_dim:, emb_dim:] = dataB[:, emb_dim:]\n",
    "\n",
    "%timeit spspmm(indexA, valueA, indexB, valueB, 2 * emb_dim, 2 * emb_dim, 2 * emb_dim)\n",
    "%timeit matrixA @ matrixB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "sl2 = sl.SparseLinear(input_dim, output_dim, connectivity=connections).cuda()\n",
    "fc2 = nn.Linear(input_dim, output_dim).cuda()\n",
    "\n",
    "t1, t2 = 100, 50\n",
    "data1 = torch.rand((4, t1, emb_dim)).to(device)\n",
    "data2 = torch.rand((4, t2, emb_dim)).to(device)\n",
    "data = torch.zeros((4, t1 + t2, input_dim)).to(device)\n",
    "data[:, :t1, :emb_dim] = data1\n",
    "data[:, t1:, emb_dim:] = data2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978 µs ± 21.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit y = sl2(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.48 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "71.7 µs ± 53.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit y = fc2(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}